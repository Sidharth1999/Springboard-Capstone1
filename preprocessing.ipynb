{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python371jvsc74a57bd0597be375a0af02eb3c3d3d6fe8c370a08fb7e001d2d85e430e9722657aedcaff",
   "display_name": "Python 3.7.1 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are 2 major preprocessing steps to be performed before fitting any clustering models -\n",
    "\n",
    "#1)Removing outliers. Outliers can be very damaging in clustering algorithms like K-Means because even a single outlier can dramatically influence a cluster center. \n",
    "\n",
    "#2)Normalizing the data. Again, distance-sensitive algorithms like K-Means treat observations as vectors in a vector space with an implied inner product like Euclidean distance. This means all features need to have equal weights in feature vectors and that means normalizing their scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/EDACollegeScorecard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7803\n2577\n"
     ]
    }
   ],
   "source": [
    "#First, lets remove all observations with values in a column that are more than 3 standard deviations away from that column's mean\n",
    "#Source: 'https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame'\n",
    "from scipy import stats\n",
    "\n",
    "print(len(df))\n",
    "numeric_df = df.select_dtypes(\"number\") #We may as well drop the INSTURL column, it does not add any meaningful value to the dataset\n",
    "new_df = numeric_df[(np.abs(stats.zscore(numeric_df)) < 3).all(axis=1)]\n",
    "print(len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#That seems to drop way too much data, more than half. Perhaps we should be more sparing about our standard deviation threshold ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7803\n6295\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "numeric_df = df.select_dtypes(\"number\") #We may as well drop the INSTURL column, it does not add any meaningful value to the dataset\n",
    "new_df = numeric_df[(np.abs(stats.zscore(numeric_df)) < 8).all(axis=1)]\n",
    "print(len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Around 1600 observations were dropped. It's a decent amount of data, but we still have more than 6000 observations to build a potentially more robust clustering model. If the models dont perform well, we can come back to this step and tweak the SD threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           0         1             2         3         4    5    6     7    \\\n",
       "0     0.000000  0.000000  0.000000e+00  0.000461  0.001188  0.0  1.0  0.00   \n",
       "1     0.000128  0.000128  5.603398e-07  0.226011  0.582669  0.0  1.0  0.00   \n",
       "2     0.000256  0.000256  8.923931e-07  0.000489  0.001261  0.0  1.0  0.00   \n",
       "3     0.000641  0.000641  2.013073e-06  0.000038  0.000097  0.0  1.0  0.00   \n",
       "4     0.000769  0.000769  3.092246e-06  0.000047  0.000121  0.0  1.0  0.00   \n",
       "...        ...       ...           ...       ...       ...  ...  ...   ...   \n",
       "6290  0.999487  0.999487  9.999999e-01  0.005343  0.013772  0.0  0.0  0.05   \n",
       "6291  0.999615  0.999615  9.999999e-01  0.005342  0.013772  0.0  0.0  0.05   \n",
       "6292  0.999744  0.999744  1.000000e+00  0.005343  0.013772  0.0  0.0  0.05   \n",
       "6293  0.999872  0.999872  1.000000e+00  0.005342  0.013772  0.0  1.0  0.05   \n",
       "6294  1.000000  1.000000  1.000000e+00  0.005342  0.013772  0.0  0.0  0.05   \n",
       "\n",
       "      8         9    ...       213       214       215       216       217  \\\n",
       "0     0.0  0.000000  ...  0.711863  0.730729  0.751029  0.753725  0.720400   \n",
       "1     0.5  0.000000  ...  0.579620  0.659221  0.677674  0.611111  0.583138   \n",
       "2     0.0  0.000000  ...  0.708883  0.794698  0.797386  0.788247  0.726713   \n",
       "3     0.0  0.000000  ...  0.324754  0.505528  0.662853  0.501976  0.342870   \n",
       "4     0.0  0.000000  ...  0.697118  0.763222  0.857456  0.877229  0.724479   \n",
       "...   ...       ...  ...       ...       ...       ...       ...       ...   \n",
       "6290  0.0  0.155844  ...  0.394902  0.461457  0.452632  0.451434  0.402464   \n",
       "6291  0.0  0.155844  ...  0.394902  0.461457  0.452632  0.451434  0.402464   \n",
       "6292  0.0  0.155844  ...  0.394902  0.461457  0.452632  0.451434  0.402464   \n",
       "6293  0.0  0.155844  ...  0.394902  0.461457  0.452632  0.451434  0.402464   \n",
       "6294  0.0  0.155844  ...  0.394902  0.461457  0.452632  0.451434  0.402464   \n",
       "\n",
       "           218       219       220       221       222  \n",
       "0     0.760188  0.745171  0.744139  0.736720  0.759968  \n",
       "1     0.736207  0.585495  0.672211  0.610722  0.642391  \n",
       "2     0.825786  0.753531  0.789209  0.764913  0.787138  \n",
       "3     0.663019  0.388141  0.444554  0.427713  0.425513  \n",
       "4     0.792340  0.735767  0.801658  0.766960  0.757499  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "6290  0.540125  0.457405  0.437099  0.459960  0.484178  \n",
       "6291  0.540125  0.457405  0.437099  0.459960  0.484178  \n",
       "6292  0.540125  0.457405  0.437099  0.459960  0.484178  \n",
       "6293  0.540125  0.457405  0.437099  0.459960  0.484178  \n",
       "6294  0.540125  0.457405  0.437099  0.459960  0.484178  \n",
       "\n",
       "[6295 rows x 223 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>213</th>\n      <th>214</th>\n      <th>215</th>\n      <th>216</th>\n      <th>217</th>\n      <th>218</th>\n      <th>219</th>\n      <th>220</th>\n      <th>221</th>\n      <th>222</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>0.000461</td>\n      <td>0.001188</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.711863</td>\n      <td>0.730729</td>\n      <td>0.751029</td>\n      <td>0.753725</td>\n      <td>0.720400</td>\n      <td>0.760188</td>\n      <td>0.745171</td>\n      <td>0.744139</td>\n      <td>0.736720</td>\n      <td>0.759968</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000128</td>\n      <td>0.000128</td>\n      <td>5.603398e-07</td>\n      <td>0.226011</td>\n      <td>0.582669</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.579620</td>\n      <td>0.659221</td>\n      <td>0.677674</td>\n      <td>0.611111</td>\n      <td>0.583138</td>\n      <td>0.736207</td>\n      <td>0.585495</td>\n      <td>0.672211</td>\n      <td>0.610722</td>\n      <td>0.642391</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000256</td>\n      <td>0.000256</td>\n      <td>8.923931e-07</td>\n      <td>0.000489</td>\n      <td>0.001261</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.708883</td>\n      <td>0.794698</td>\n      <td>0.797386</td>\n      <td>0.788247</td>\n      <td>0.726713</td>\n      <td>0.825786</td>\n      <td>0.753531</td>\n      <td>0.789209</td>\n      <td>0.764913</td>\n      <td>0.787138</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000641</td>\n      <td>0.000641</td>\n      <td>2.013073e-06</td>\n      <td>0.000038</td>\n      <td>0.000097</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.324754</td>\n      <td>0.505528</td>\n      <td>0.662853</td>\n      <td>0.501976</td>\n      <td>0.342870</td>\n      <td>0.663019</td>\n      <td>0.388141</td>\n      <td>0.444554</td>\n      <td>0.427713</td>\n      <td>0.425513</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000769</td>\n      <td>0.000769</td>\n      <td>3.092246e-06</td>\n      <td>0.000047</td>\n      <td>0.000121</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.697118</td>\n      <td>0.763222</td>\n      <td>0.857456</td>\n      <td>0.877229</td>\n      <td>0.724479</td>\n      <td>0.792340</td>\n      <td>0.735767</td>\n      <td>0.801658</td>\n      <td>0.766960</td>\n      <td>0.757499</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6290</th>\n      <td>0.999487</td>\n      <td>0.999487</td>\n      <td>9.999999e-01</td>\n      <td>0.005343</td>\n      <td>0.013772</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.05</td>\n      <td>0.0</td>\n      <td>0.155844</td>\n      <td>...</td>\n      <td>0.394902</td>\n      <td>0.461457</td>\n      <td>0.452632</td>\n      <td>0.451434</td>\n      <td>0.402464</td>\n      <td>0.540125</td>\n      <td>0.457405</td>\n      <td>0.437099</td>\n      <td>0.459960</td>\n      <td>0.484178</td>\n    </tr>\n    <tr>\n      <th>6291</th>\n      <td>0.999615</td>\n      <td>0.999615</td>\n      <td>9.999999e-01</td>\n      <td>0.005342</td>\n      <td>0.013772</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.05</td>\n      <td>0.0</td>\n      <td>0.155844</td>\n      <td>...</td>\n      <td>0.394902</td>\n      <td>0.461457</td>\n      <td>0.452632</td>\n      <td>0.451434</td>\n      <td>0.402464</td>\n      <td>0.540125</td>\n      <td>0.457405</td>\n      <td>0.437099</td>\n      <td>0.459960</td>\n      <td>0.484178</td>\n    </tr>\n    <tr>\n      <th>6292</th>\n      <td>0.999744</td>\n      <td>0.999744</td>\n      <td>1.000000e+00</td>\n      <td>0.005343</td>\n      <td>0.013772</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.05</td>\n      <td>0.0</td>\n      <td>0.155844</td>\n      <td>...</td>\n      <td>0.394902</td>\n      <td>0.461457</td>\n      <td>0.452632</td>\n      <td>0.451434</td>\n      <td>0.402464</td>\n      <td>0.540125</td>\n      <td>0.457405</td>\n      <td>0.437099</td>\n      <td>0.459960</td>\n      <td>0.484178</td>\n    </tr>\n    <tr>\n      <th>6293</th>\n      <td>0.999872</td>\n      <td>0.999872</td>\n      <td>1.000000e+00</td>\n      <td>0.005342</td>\n      <td>0.013772</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.05</td>\n      <td>0.0</td>\n      <td>0.155844</td>\n      <td>...</td>\n      <td>0.394902</td>\n      <td>0.461457</td>\n      <td>0.452632</td>\n      <td>0.451434</td>\n      <td>0.402464</td>\n      <td>0.540125</td>\n      <td>0.457405</td>\n      <td>0.437099</td>\n      <td>0.459960</td>\n      <td>0.484178</td>\n    </tr>\n    <tr>\n      <th>6294</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000e+00</td>\n      <td>0.005342</td>\n      <td>0.013772</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.05</td>\n      <td>0.0</td>\n      <td>0.155844</td>\n      <td>...</td>\n      <td>0.394902</td>\n      <td>0.461457</td>\n      <td>0.452632</td>\n      <td>0.451434</td>\n      <td>0.402464</td>\n      <td>0.540125</td>\n      <td>0.457405</td>\n      <td>0.437099</td>\n      <td>0.459960</td>\n      <td>0.484178</td>\n    </tr>\n  </tbody>\n</table>\n<p>6295 rows × 223 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "#Now, we need to normalize the data\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x = new_df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "normalized_df = pd.DataFrame(x_scaled)\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is no reason to split this dataframe into a train and test set, since this is an unsupervised learning task. We want our clustering algorithms to use as much data as possible. So we are done with preprocessing, all's that's left is to save the df\n",
    "\n",
    "normalized_df.to_csv('./data/PreprocessedCollegeScorecard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}